[2024-04-25T13:32:56.805+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer manual__2024-04-25T13:32:51.322992+00:00 [queued]>
[2024-04-25T13:32:56.825+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer manual__2024-04-25T13:32:51.322992+00:00 [queued]>
[2024-04-25T13:32:56.825+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2024-04-25T13:32:56.860+0000] {taskinstance.py:1382} INFO - Executing <Task(DockerOperator): pyspark_consumer> on 2024-04-25 13:32:51.322992+00:00
[2024-04-25T13:32:56.872+0000] {standard_task_runner.py:57} INFO - Started process 3073 to run task
[2024-04-25T13:32:56.879+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'kafka_spark_dag', 'pyspark_consumer', 'manual__2024-04-25T13:32:51.322992+00:00', '--job-id', '21', '--raw', '--subdir', 'DAGS_FOLDER/dag_kafka_spark.py', '--cfg-path', '/tmp/tmpmgg3dmry']
[2024-04-25T13:32:56.885+0000] {standard_task_runner.py:85} INFO - Job 21: Subtask pyspark_consumer
[2024-04-25T13:32:56.969+0000] {task_command.py:416} INFO - Running <TaskInstance: kafka_spark_dag.pyspark_consumer manual__2024-04-25T13:32:51.322992+00:00 [running]> on host fe238196d269
[2024-04-25T13:32:57.079+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='kafka_spark_dag' AIRFLOW_CTX_TASK_ID='pyspark_consumer' AIRFLOW_CTX_EXECUTION_DATE='2024-04-25T13:32:51.322992+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-04-25T13:32:51.322992+00:00'
[2024-04-25T13:32:57.114+0000] {docker.py:343} INFO - Starting docker container from image rappel-conso/spark:latest
[2024-04-25T13:32:57.119+0000] {docker.py:351} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-04-25T13:32:57.553+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:32:57.55 [0m[38;5;2mINFO [0m ==>
[2024-04-25T13:32:57.556+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:32:57.55 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2024-04-25T13:32:57.558+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:32:57.55 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2024-04-25T13:32:57.560+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:32:57.55 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
[2024-04-25T13:32:57.562+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:32:57.56 [0m[38;5;2mINFO [0m ==>
[2024-04-25T13:32:57.574+0000] {docker.py:413} INFO - 
[2024-04-25T13:32:59.995+0000] {docker.py:413} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-04-25T13:33:00.091+0000] {docker.py:413} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2024-04-25T13:33:00.099+0000] {docker.py:413} INFO - org.postgresql#postgresql added as a dependency
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-04-25T13:33:00.101+0000] {docker.py:413} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-f5195496-9118-4a56-b8c3-c5d1dd472023;1.0
	confs: [default]
[2024-04-25T13:33:00.789+0000] {docker.py:413} INFO - found org.postgresql#postgresql;42.5.4 in central
[2024-04-25T13:33:00.857+0000] {docker.py:413} INFO - found org.checkerframework#checker-qual;3.5.0 in central
[2024-04-25T13:33:01.907+0000] {docker.py:413} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
[2024-04-25T13:33:02.314+0000] {docker.py:413} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2024-04-25T13:33:02.389+0000] {docker.py:413} INFO - found org.apache.kafka#kafka-clients;3.4.1 in central
[2024-04-25T13:33:02.460+0000] {docker.py:413} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-04-25T13:33:02.520+0000] {docker.py:413} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-04-25T13:33:02.895+0000] {docker.py:413} INFO - found org.slf4j#slf4j-api;2.0.7 in central
[2024-04-25T13:33:03.680+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-04-25T13:33:03.807+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-04-25T13:33:04.061+0000] {docker.py:413} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-04-25T13:33:04.110+0000] {docker.py:413} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-04-25T13:33:04.907+0000] {docker.py:413} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-04-25T13:33:04.935+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.4/postgresql-42.5.4.jar ...
[2024-04-25T13:33:05.055+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.5.4!postgresql.jar (133ms)
[2024-04-25T13:33:05.070+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
[2024-04-25T13:33:05.117+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (60ms)
[2024-04-25T13:33:05.132+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...
[2024-04-25T13:33:05.157+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (38ms)
[2024-04-25T13:33:05.172+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
[2024-04-25T13:33:05.190+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (31ms)
[2024-04-25T13:33:05.212+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[2024-04-25T13:33:05.427+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (235ms)
[2024-04-25T13:33:05.440+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2024-04-25T13:33:05.454+0000] {docker.py:413} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (25ms)
[2024-04-25T13:33:05.469+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-04-25T13:33:05.485+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (29ms)
[2024-04-25T13:33:05.498+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2024-04-25T13:33:05.906+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (420ms)
[2024-04-25T13:33:05.921+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-04-25T13:33:05.943+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (36ms)
[2024-04-25T13:33:05.960+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2024-04-25T13:33:06.008+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (64ms)
[2024-04-25T13:33:06.020+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2024-04-25T13:33:06.035+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (26ms)
[2024-04-25T13:33:06.049+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2024-04-25T13:33:06.314+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (278ms)
[2024-04-25T13:33:06.327+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2024-04-25T13:33:06.341+0000] {docker.py:413} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (25ms)
[2024-04-25T13:33:06.342+0000] {docker.py:413} INFO - :: resolution report :: resolve 4819ms :: artifacts dl 1421ms
[2024-04-25T13:33:06.342+0000] {docker.py:413} INFO - :: modules in use:
[2024-04-25T13:33:06.344+0000] {docker.py:413} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
[2024-04-25T13:33:06.345+0000] {docker.py:413} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
	org.checkerframework#checker-qual;3.5.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.5.4 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2024-04-25T13:33:06.357+0000] {docker.py:413} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-f5195496-9118-4a56-b8c3-c5d1dd472023
[2024-04-25T13:33:06.358+0000] {docker.py:413} INFO - confs: [default]
[2024-04-25T13:33:06.478+0000] {docker.py:413} INFO - 13 artifacts copied, 0 already retrieved (58001kB/120ms)
[2024-04-25T13:33:06.753+0000] {docker.py:413} INFO - 24/04/25 13:33:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-04-25T13:33:08.344+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SparkContext: Running Spark version 3.5.0
[2024-04-25T13:33:08.346+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SparkContext: OS info Linux, 6.5.0-1016-gcp, amd64
[2024-04-25T13:33:08.347+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SparkContext: Java version 17.0.10
[2024-04-25T13:33:08.394+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO ResourceUtils: ==============================================================
[2024-04-25T13:33:08.396+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-04-25T13:33:08.396+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO ResourceUtils: ==============================================================
[2024-04-25T13:33:08.397+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SparkContext: Submitted application: PostgreSQL Connection with PySpark
[2024-04-25T13:33:08.426+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-04-25T13:33:08.438+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO ResourceProfile: Limiting resource is cpu
[2024-04-25T13:33:08.440+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-04-25T13:33:08.512+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SecurityManager: Changing view acls to: spark
[2024-04-25T13:33:08.512+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SecurityManager: Changing modify acls to: spark
[2024-04-25T13:33:08.513+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SecurityManager: Changing view acls groups to:
[2024-04-25T13:33:08.514+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SecurityManager: Changing modify acls groups to:
[2024-04-25T13:33:08.514+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2024-04-25T13:33:08.861+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO Utils: Successfully started service 'sparkDriver' on port 42117.
[2024-04-25T13:33:08.908+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SparkEnv: Registering MapOutputTracker
[2024-04-25T13:33:08.966+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO SparkEnv: Registering BlockManagerMaster
[2024-04-25T13:33:08.994+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-04-25T13:33:08.995+0000] {docker.py:413} INFO - 24/04/25 13:33:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-04-25T13:33:09.001+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-04-25T13:33:09.032+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a726461d-d3a0-4b94-a319-337a71a36629
[2024-04-25T13:33:09.056+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-04-25T13:33:09.080+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-04-25T13:33:09.286+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-04-25T13:33:09.366+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-04-25T13:33:09.434+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar at spark://localhost:42117/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1714051988330
[2024-04-25T13:33:09.435+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://localhost:42117/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.436+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar at spark://localhost:42117/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.436+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://localhost:42117/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1714051988330
24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://localhost:42117/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714051988330
[2024-04-25T13:33:09.436+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://localhost:42117/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.437+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://localhost:42117/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714051988330
[2024-04-25T13:33:09.437+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://localhost:42117/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714051988330
[2024-04-25T13:33:09.437+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://localhost:42117/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.438+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://localhost:42117/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714051988330
[2024-04-25T13:33:09.438+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://localhost:42117/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714051988330
[2024-04-25T13:33:09.438+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://localhost:42117/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714051988330
[2024-04-25T13:33:09.439+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://localhost:42117/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714051988330
[2024-04-25T13:33:09.445+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1714051988330
[2024-04-25T13:33:09.447+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.postgresql_postgresql-42.5.4.jar
[2024-04-25T13:33:09.463+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.465+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2024-04-25T13:33:09.474+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.475+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.checkerframework_checker-qual-3.5.0.jar
[2024-04-25T13:33:09.484+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.485+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2024-04-25T13:33:09.493+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714051988330
[2024-04-25T13:33:09.494+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.kafka_kafka-clients-3.4.1.jar
[2024-04-25T13:33:09.510+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.511+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-04-25T13:33:09.519+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714051988330
[2024-04-25T13:33:09.520+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.commons_commons-pool2-2.11.1.jar
[2024-04-25T13:33:09.528+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714051988330
[2024-04-25T13:33:09.529+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-04-25T13:33:09.581+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.582+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.lz4_lz4-java-1.8.0.jar
[2024-04-25T13:33:09.589+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714051988330
[2024-04-25T13:33:09.590+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-04-25T13:33:09.599+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714051988330
[2024-04-25T13:33:09.600+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.slf4j_slf4j-api-2.0.7.jar
[2024-04-25T13:33:09.606+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714051988330
[2024-04-25T13:33:09.607+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-04-25T13:33:09.635+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714051988330
[2024-04-25T13:33:09.635+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/commons-logging_commons-logging-1.1.3.jar
[2024-04-25T13:33:09.721+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Starting executor ID driver on host localhost
[2024-04-25T13:33:09.722+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: OS info Linux, 6.5.0-1016-gcp, amd64
[2024-04-25T13:33:09.722+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Java version 17.0.10
[2024-04-25T13:33:09.730+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-04-25T13:33:09.731+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@382f6474 for default.
[2024-04-25T13:33:09.746+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.770+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2024-04-25T13:33:09.775+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714051988330
[2024-04-25T13:33:09.818+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-04-25T13:33:09.824+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714051988330
[2024-04-25T13:33:09.825+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.commons_commons-pool2-2.11.1.jar
[2024-04-25T13:33:09.830+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1714051988330
[2024-04-25T13:33:09.834+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.postgresql_postgresql-42.5.4.jar
[2024-04-25T13:33:09.840+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714051988330
[2024-04-25T13:33:09.841+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.slf4j_slf4j-api-2.0.7.jar
[2024-04-25T13:33:09.848+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714051988330
[2024-04-25T13:33:09.859+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.kafka_kafka-clients-3.4.1.jar
[2024-04-25T13:33:09.866+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.867+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-04-25T13:33:09.873+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.876+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.lz4_lz4-java-1.8.0.jar
[2024-04-25T13:33:09.883+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.885+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2024-04-25T13:33:09.891+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714051988330
[2024-04-25T13:33:09.920+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-04-25T13:33:09.925+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714051988330
[2024-04-25T13:33:09.926+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/commons-logging_commons-logging-1.1.3.jar
[2024-04-25T13:33:09.932+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:09.933+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.checkerframework_checker-qual-3.5.0.jar
[2024-04-25T13:33:09.939+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714051988330
[2024-04-25T13:33:09.942+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-04-25T13:33:09.952+0000] {docker.py:413} INFO - 24/04/25 13:33:09 INFO Executor: Fetching spark://localhost:42117/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:10.006+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:42117 after 38 ms (0 ms spent in bootstraps)
[2024-04-25T13:33:10.014+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp4426631490449226748.tmp
[2024-04-25T13:33:10.055+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp4426631490449226748.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2024-04-25T13:33:10.065+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714051988330
[2024-04-25T13:33:10.066+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp9693450107762077301.tmp
[2024-04-25T13:33:10.075+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp9693450107762077301.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.lz4_lz4-java-1.8.0.jar
[2024-04-25T13:33:10.084+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.lz4_lz4-java-1.8.0.jar to class loader default
[2024-04-25T13:33:10.085+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714051988330
[2024-04-25T13:33:10.086+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp17663933962343033297.tmp
[2024-04-25T13:33:10.114+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp17663933962343033297.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-04-25T13:33:10.133+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
[2024-04-25T13:33:10.134+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714051988330
[2024-04-25T13:33:10.135+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp6941328167712279756.tmp
[2024-04-25T13:33:10.266+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp6941328167712279756.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-04-25T13:33:10.275+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
[2024-04-25T13:33:10.276+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1714051988330
[2024-04-25T13:33:10.277+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.checkerframework_checker-qual-3.5.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp5905673169454240605.tmp
[2024-04-25T13:33:10.280+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp5905673169454240605.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.checkerframework_checker-qual-3.5.0.jar
[2024-04-25T13:33:10.287+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.checkerframework_checker-qual-3.5.0.jar to class loader default
[2024-04-25T13:33:10.288+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1714051988330
24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp8258875343352763183.tmp
[2024-04-25T13:33:10.291+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp8258875343352763183.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2024-04-25T13:33:10.298+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
[2024-04-25T13:33:10.299+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714051988330
[2024-04-25T13:33:10.299+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp8483394346466208848.tmp
[2024-04-25T13:33:10.302+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp8483394346466208848.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.commons_commons-pool2-2.11.1.jar
[2024-04-25T13:33:10.309+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2024-04-25T13:33:10.309+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714051988330
[2024-04-25T13:33:10.310+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp13887937490123606670.tmp
[2024-04-25T13:33:10.312+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp13887937490123606670.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-04-25T13:33:10.318+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
[2024-04-25T13:33:10.318+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714051988330
[2024-04-25T13:33:10.319+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp11438361306701615688.tmp
[2024-04-25T13:33:10.321+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp11438361306701615688.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/commons-logging_commons-logging-1.1.3.jar
[2024-04-25T13:33:10.327+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/commons-logging_commons-logging-1.1.3.jar to class loader default
[2024-04-25T13:33:10.328+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714051988330
[2024-04-25T13:33:10.329+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp2263622902371253579.tmp
[2024-04-25T13:33:10.564+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp2263622902371253579.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-04-25T13:33:10.575+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
[2024-04-25T13:33:10.575+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714051988330
[2024-04-25T13:33:10.576+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp5536187662933213948.tmp
[2024-04-25T13:33:10.577+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp5536187662933213948.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.slf4j_slf4j-api-2.0.7.jar
[2024-04-25T13:33:10.584+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.slf4j_slf4j-api-2.0.7.jar to class loader default
[2024-04-25T13:33:10.584+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1714051988330
[2024-04-25T13:33:10.585+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.postgresql_postgresql-42.5.4.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp4051521458197424873.tmp
[2024-04-25T13:33:10.592+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp4051521458197424873.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.postgresql_postgresql-42.5.4.jar
[2024-04-25T13:33:10.600+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.postgresql_postgresql-42.5.4.jar to class loader default
[2024-04-25T13:33:10.601+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Fetching spark://localhost:42117/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714051988330
[2024-04-25T13:33:10.602+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Fetching spark://localhost:42117/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp11736848669096609045.tmp
[2024-04-25T13:33:10.623+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/fetchFileTemp11736848669096609045.tmp has been previously copied to /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.kafka_kafka-clients-3.4.1.jar
[2024-04-25T13:33:10.634+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Executor: Adding file:/tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/userFiles-215f934e-6317-4570-b79e-6b42d4ef6d9f/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
[2024-04-25T13:33:10.648+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45005.
[2024-04-25T13:33:10.649+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO NettyBlockTransferService: Server created on localhost:45005
[2024-04-25T13:33:10.652+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-04-25T13:33:10.660+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 45005, None)
[2024-04-25T13:33:10.666+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO BlockManagerMasterEndpoint: Registering block manager localhost:45005 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 45005, None)
[2024-04-25T13:33:10.669+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 45005, None)
[2024-04-25T13:33:10.671+0000] {docker.py:413} INFO - 24/04/25 13:33:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 45005, None)
[2024-04-25T13:33:11.174+0000] {docker.py:413} INFO - 2024-04-25 13:33:11,173:create_spark_session:INFO:Spark session created successfully
[2024-04-25T13:33:11.182+0000] {docker.py:413} INFO - 24/04/25 13:33:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-04-25T13:33:11.185+0000] {docker.py:413} INFO - 24/04/25 13:33:11 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2024-04-25T13:33:13.163+0000] {docker.py:413} INFO - 2024-04-25 13:33:13,162:create_initial_dataframe:INFO:Initial dataframe created successfully
[2024-04-25T13:33:14.120+0000] {docker.py:413} INFO - 2024-04-25 13:33:14,119:start_streaming:INFO:Start streaming ...
[2024-04-25T13:33:14.441+0000] {docker.py:413} INFO - 2024-04-25 13:33:14,440:run:INFO:Callback Server Starting
[2024-04-25T13:33:14.442+0000] {docker.py:413} INFO - 2024-04-25 13:33:14,441:run:INFO:Socket listening on ('127.0.0.1', 41123)
[2024-04-25T13:33:14.464+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-04-25T13:33:14.497+0000] {docker.py:413} INFO - 24/04/25 13:33:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-04-25T13:33:14.522+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09 resolved to file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09.
[2024-04-25T13:33:14.522+0000] {docker.py:413} INFO - 24/04/25 13:33:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-04-25T13:33:14.613+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/metadata using temp file file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/.metadata.c568d1e7-9b46-48c9-a7e8-811c074e90ed.tmp
[2024-04-25T13:33:14.719+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/.metadata.c568d1e7-9b46-48c9-a7e8-811c074e90ed.tmp to file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/metadata
[2024-04-25T13:33:14.764+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO MicroBatchExecution: Starting [id = 0744d6c1-ff49-4e4b-a6e1-1613983062db, runId = 5bcdc2a4-dc27-4873-948e-2bf8e3e9a4b4]. Use file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09 to store the query checkpoint.
[2024-04-25T13:33:14.778+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2eef6209] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2698573b]
[2024-04-25T13:33:14.817+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO OffsetSeqLog: BatchIds found from listing:
[2024-04-25T13:33:14.820+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO OffsetSeqLog: BatchIds found from listing:
[2024-04-25T13:33:14.820+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO MicroBatchExecution: Starting new streaming query.
[2024-04-25T13:33:14.824+0000] {docker.py:413} INFO - 24/04/25 13:33:14 INFO MicroBatchExecution: Stream started from {}
[2024-04-25T13:33:15.237+0000] {docker.py:413} INFO - 24/04/25 13:33:15 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
[2024-04-25T13:33:15.337+0000] {docker.py:413} INFO - 24/04/25 13:33:15 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2024-04-25T13:33:15.340+0000] {docker.py:413} INFO - 24/04/25 13:33:15 INFO AppInfoParser: Kafka version: 3.4.1
24/04/25 13:33:15 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2024-04-25T13:33:15.340+0000] {docker.py:413} INFO - 24/04/25 13:33:15 INFO AppInfoParser: Kafka startTimeMs: 1714051995337
[2024-04-25T13:33:15.862+0000] {docker.py:413} INFO - 24/04/25 13:33:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/sources/0/0 using temp file file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/sources/0/.0.d4fb65c7-9ca7-430f-b14a-e25c07a47b91.tmp
[2024-04-25T13:33:15.912+0000] {docker.py:413} INFO - 24/04/25 13:33:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/sources/0/.0.d4fb65c7-9ca7-430f-b14a-e25c07a47b91.tmp to file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/sources/0/0
[2024-04-25T13:33:15.914+0000] {docker.py:413} INFO - 24/04/25 13:33:15 INFO KafkaMicroBatchStream: Initial offsets: {"rappel_conso":{"0":0}}
[2024-04-25T13:33:15.948+0000] {docker.py:413} INFO - 24/04/25 13:33:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/offsets/0 using temp file file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/offsets/.0.feb9adf8-1c6d-442b-b35e-d77023a8c482.tmp
[2024-04-25T13:33:16.002+0000] {docker.py:413} INFO - 24/04/25 13:33:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/offsets/.0.feb9adf8-1c6d-442b-b35e-d77023a8c482.tmp to file:/tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09/offsets/0
[2024-04-25T13:33:16.003+0000] {docker.py:413} INFO - 24/04/25 13:33:16 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1714051995932,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-04-25T13:33:16.552+0000] {docker.py:413} INFO - 24/04/25 13:33:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-04-25T13:33:16.625+0000] {docker.py:413} INFO - 24/04/25 13:33:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-04-25T13:33:16.732+0000] {docker.py:413} INFO - 24/04/25 13:33:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-04-25T13:33:16.735+0000] {docker.py:413} INFO - 24/04/25 13:33:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-04-25T13:33:16.755+0000] {docker.py:413} INFO - 24/04/25 13:33:16 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2024-04-25T13:33:17.246+0000] {docker.py:413} INFO - 24/04/25 13:33:17 INFO CodeGenerator: Code generated in 290.7887 ms
[2024-04-25T13:33:17.436+0000] {docker.py:413} INFO - 2024-04-25 13:33:17,434:wait_for_commands:INFO:Python Server ready to receive messages
[2024-04-25T13:33:17.437+0000] {docker.py:413} INFO - 2024-04-25 13:33:17,436:wait_for_commands:INFO:Received command c on object id p0
[2024-04-25T13:33:18.022+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO CodeGenerator: Code generated in 124.010133 ms
[2024-04-25T13:33:18.065+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO CodeGenerator: Code generated in 27.392847 ms
[2024-04-25T13:33:18.117+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO CodeGenerator: Code generated in 42.717668 ms
[2024-04-25T13:33:18.355+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO CodeGenerator: Code generated in 19.906646 ms
[2024-04-25T13:33:18.373+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO CodeGenerator: Code generated in 12.565562 ms
[2024-04-25T13:33:18.519+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-04-25T13:33:18.545+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Registering RDD 11 (start at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2024-04-25T13:33:18.561+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Registering RDD 7 (start at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2024-04-25T13:33:18.566+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 200 output partitions
[2024-04-25T13:33:18.567+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
[2024-04-25T13:33:18.569+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[2024-04-25T13:33:18.574+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[2024-04-25T13:33:18.581+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-04-25T13:33:18.652+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.0 KiB, free 434.4 MiB)
[2024-04-25T13:33:18.711+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.4 MiB)
[2024-04-25T13:33:18.714+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:45005 (size: 7.4 KiB, free: 434.4 MiB)
[2024-04-25T13:33:18.720+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
[2024-04-25T13:33:18.745+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-04-25T13:33:18.747+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-04-25T13:33:18.774+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-04-25T13:33:18.811+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 52.0 KiB, free 434.3 MiB)
[2024-04-25T13:33:18.813+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 434.3 MiB)
[2024-04-25T13:33:18.814+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:45005 (size: 17.8 KiB, free: 434.4 MiB)
[2024-04-25T13:33:18.816+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2024-04-25T13:33:18.818+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-04-25T13:33:18.819+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-04-25T13:33:18.840+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 9967 bytes)
[2024-04-25T13:33:18.874+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11060 bytes)
[2024-04-25T13:33:18.878+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-04-25T13:33:18.879+0000] {docker.py:413} INFO - 24/04/25 13:33:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-04-25T13:33:19.147+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO CodeGenerator: Code generated in 23.301443 ms
[2024-04-25T13:33:19.150+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO CodeGenerator: Code generated in 106.387087 ms
[2024-04-25T13:33:19.194+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO CodeGenerator: Code generated in 25.221269 ms
[2024-04-25T13:33:19.211+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO JDBCRDD: closed connection
[2024-04-25T13:33:19.224+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO CodeGenerator: Code generated in 66.193936 ms
[2024-04-25T13:33:19.271+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1857 bytes result sent to driver
[2024-04-25T13:33:19.282+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO CodeGenerator: Code generated in 54.557968 ms
[2024-04-25T13:33:19.292+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=rappel_conso-0 fromOffset=0 untilOffset=21156, for query queryId=0744d6c1-ff49-4e4b-a6e1-1613983062db batchId=0 taskId=1 partitionId=0
[2024-04-25T13:33:19.301+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 475 ms on localhost (executor driver) (1/1)
[2024-04-25T13:33:19.320+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO DAGScheduler: ShuffleMapStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.724 s
[2024-04-25T13:33:19.322+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO DAGScheduler: looking for newly runnable stages
[2024-04-25T13:33:19.323+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-04-25T13:33:19.324+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
[2024-04-25T13:33:19.325+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2024-04-25T13:33:19.326+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO DAGScheduler: failed: Set()
[2024-04-25T13:33:19.375+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO CodeGenerator: Code generated in 16.370692 ms
[2024-04-25T13:33:19.409+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO CodeGenerator: Code generated in 20.892504 ms
[2024-04-25T13:33:19.433+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-04-25T13:33:19.493+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO AppInfoParser: Kafka version: 3.4.1
[2024-04-25T13:33:19.494+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2024-04-25T13:33:19.494+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO AppInfoParser: Kafka startTimeMs: 1714051999492
[2024-04-25T13:33:19.498+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Assigned to partition(s): rappel_conso-0
[2024-04-25T13:33:19.509+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 0 for partition rappel_conso-0
[2024-04-25T13:33:19.520+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting the last seen epoch of partition rappel_conso-0 to 0 since the associated topicId changed from null to PcKLkzc3SEixfiye3_5m0w
[2024-04-25T13:33:19.521+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Cluster ID: Pe_yHPgeSfOGp_qQkVz9dQ
[2024-04-25T13:33:19.583+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:45005 in memory (size: 7.4 KiB, free: 434.4 MiB)
[2024-04-25T13:33:19.609+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:19.616+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:19.617+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:19.618+0000] {docker.py:413} INFO - 24/04/25 13:33:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.071+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 500 for partition rappel_conso-0
[2024-04-25T13:33:20.092+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.100+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.100+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.104+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.164+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 721 for partition rappel_conso-0
[2024-04-25T13:33:20.169+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.172+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.173+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.176+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.276+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 1221 for partition rappel_conso-0
[2024-04-25T13:33:20.282+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.355+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.357+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.363+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.391+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 1332 for partition rappel_conso-0
[2024-04-25T13:33:20.396+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.407+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.408+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.412+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.628+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 1832 for partition rappel_conso-0
[2024-04-25T13:33:20.666+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.681+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.682+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.693+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.708+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 1893 for partition rappel_conso-0
[2024-04-25T13:33:20.709+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.711+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.712+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.716+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.781+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 2393 for partition rappel_conso-0
[2024-04-25T13:33:20.786+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.804+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.805+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.806+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.816+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 2469 for partition rappel_conso-0
[2024-04-25T13:33:20.824+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.828+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.829+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.833+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.897+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 2969 for partition rappel_conso-0
[2024-04-25T13:33:20.902+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.915+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.922+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.931+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.940+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 3047 for partition rappel_conso-0
[2024-04-25T13:33:20.945+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:20.949+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:20.950+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:20.956+0000] {docker.py:413} INFO - 24/04/25 13:33:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.005+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 3547 for partition rappel_conso-0
[2024-04-25T13:33:21.008+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.015+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.016+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.019+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.024+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 3588 for partition rappel_conso-0
[2024-04-25T13:33:21.028+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.030+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.031+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.034+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.096+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 4088 for partition rappel_conso-0
[2024-04-25T13:33:21.102+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.115+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.116+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.122+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.128+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 4140 for partition rappel_conso-0
[2024-04-25T13:33:21.138+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.141+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.142+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.144+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.202+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 4640 for partition rappel_conso-0
[2024-04-25T13:33:21.207+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.215+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.218+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.221+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.234+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 4722 for partition rappel_conso-0
[2024-04-25T13:33:21.239+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.249+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.249+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.252+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.321+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 5222 for partition rappel_conso-0
[2024-04-25T13:33:21.323+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.334+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.335+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.340+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.348+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 5271 for partition rappel_conso-0
[2024-04-25T13:33:21.352+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.357+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.357+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.359+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.402+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 5771 for partition rappel_conso-0
[2024-04-25T13:33:21.407+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.418+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.418+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.422+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.428+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 5845 for partition rappel_conso-0
[2024-04-25T13:33:21.432+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.434+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.436+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.438+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.485+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 6345 for partition rappel_conso-0
[2024-04-25T13:33:21.487+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.493+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.495+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.497+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.503+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 6423 for partition rappel_conso-0
[2024-04-25T13:33:21.507+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.514+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.515+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.517+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.555+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 6923 for partition rappel_conso-0
[2024-04-25T13:33:21.558+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.565+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.566+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.567+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.574+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 7003 for partition rappel_conso-0
[2024-04-25T13:33:21.576+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.579+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.580+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.582+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.609+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 7503 for partition rappel_conso-0
[2024-04-25T13:33:21.612+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.621+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.622+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.625+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.629+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 7547 for partition rappel_conso-0
[2024-04-25T13:33:21.632+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.635+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.637+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.640+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.665+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 8047 for partition rappel_conso-0
[2024-04-25T13:33:21.668+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.677+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.678+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.682+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.685+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 8096 for partition rappel_conso-0
[2024-04-25T13:33:21.688+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.691+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.692+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.695+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.717+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 8596 for partition rappel_conso-0
[2024-04-25T13:33:21.719+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.724+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.725+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.725+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.729+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 8682 for partition rappel_conso-0
[2024-04-25T13:33:21.731+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.734+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.734+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.737+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.763+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 9182 for partition rappel_conso-0
[2024-04-25T13:33:21.766+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.773+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.774+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.775+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.776+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 9191 for partition rappel_conso-0
[2024-04-25T13:33:21.777+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.779+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.780+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.781+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.803+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 9691 for partition rappel_conso-0
[2024-04-25T13:33:21.805+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.810+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.811+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.811+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.814+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 9742 for partition rappel_conso-0
[2024-04-25T13:33:21.815+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.816+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.817+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.818+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.837+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 10242 for partition rappel_conso-0
[2024-04-25T13:33:21.840+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.846+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.847+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.848+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.851+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 10290 for partition rappel_conso-0
[2024-04-25T13:33:21.852+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.855+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.855+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.858+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.874+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 10790 for partition rappel_conso-0
[2024-04-25T13:33:21.876+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.882+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.882+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.884+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.888+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 10858 for partition rappel_conso-0
[2024-04-25T13:33:21.891+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.895+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.896+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.897+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.914+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 11358 for partition rappel_conso-0
[2024-04-25T13:33:21.918+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.922+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.923+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.925+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.928+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 11410 for partition rappel_conso-0
[2024-04-25T13:33:21.930+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.933+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.933+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.935+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.954+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 11910 for partition rappel_conso-0
[2024-04-25T13:33:21.956+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.962+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.965+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.966+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.969+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 11955 for partition rappel_conso-0
[2024-04-25T13:33:21.971+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:21.972+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.973+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:21.975+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:21.994+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 12455 for partition rappel_conso-0
[2024-04-25T13:33:21.996+0000] {docker.py:413} INFO - 24/04/25 13:33:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.002+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.002+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.004+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.007+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 12506 for partition rappel_conso-0
[2024-04-25T13:33:22.011+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.015+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.016+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.019+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.042+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 13004 for partition rappel_conso-0
[2024-04-25T13:33:22.044+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.046+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.046+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.048+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.070+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 13504 for partition rappel_conso-0
[2024-04-25T13:33:22.071+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.078+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.079+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.087+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.088+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 13530 for partition rappel_conso-0
[2024-04-25T13:33:22.088+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.093+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.094+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.098+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.122+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 14030 for partition rappel_conso-0
[2024-04-25T13:33:22.126+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.133+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.134+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.136+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.139+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 14066 for partition rappel_conso-0
[2024-04-25T13:33:22.142+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.146+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.147+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.152+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.177+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 14566 for partition rappel_conso-0
[2024-04-25T13:33:22.181+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.187+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.187+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.192+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.194+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 14575 for partition rappel_conso-0
[2024-04-25T13:33:22.198+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.200+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.201+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.202+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.231+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 15075 for partition rappel_conso-0
[2024-04-25T13:33:22.233+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.240+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.241+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.242+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.243+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 15088 for partition rappel_conso-0
[2024-04-25T13:33:22.244+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.246+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.247+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.249+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.276+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 15588 for partition rappel_conso-0
[2024-04-25T13:33:22.278+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.284+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.285+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.286+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.287+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 15612 for partition rappel_conso-0
[2024-04-25T13:33:22.289+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.291+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.291+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.292+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.318+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 16112 for partition rappel_conso-0
[2024-04-25T13:33:22.321+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.325+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.326+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.327+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.329+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 16130 for partition rappel_conso-0
[2024-04-25T13:33:22.330+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.332+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.332+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.334+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.361+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 16630 for partition rappel_conso-0
[2024-04-25T13:33:22.365+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.371+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.372+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.374+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.377+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 16646 for partition rappel_conso-0
[2024-04-25T13:33:22.379+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.381+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.382+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.384+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.413+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 17146 for partition rappel_conso-0
[2024-04-25T13:33:22.415+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.422+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.423+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.424+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.426+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 17168 for partition rappel_conso-0
[2024-04-25T13:33:22.429+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.436+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.436+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.438+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.464+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 17653 for partition rappel_conso-0
[2024-04-25T13:33:22.467+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.473+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.474+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.477+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.501+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 18135 for partition rappel_conso-0
[2024-04-25T13:33:22.505+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.514+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.516+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.523+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.570+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 18635 for partition rappel_conso-0
[2024-04-25T13:33:22.572+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.578+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.578+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.582+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.583+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 18636 for partition rappel_conso-0
[2024-04-25T13:33:22.585+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.587+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.587+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.589+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.617+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 19136 for partition rappel_conso-0
[2024-04-25T13:33:22.621+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.625+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.625+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.627+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.628+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 19141 for partition rappel_conso-0
[2024-04-25T13:33:22.631+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.633+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.633+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.635+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.688+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 19641 for partition rappel_conso-0
[2024-04-25T13:33:22.695+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.700+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.701+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.705+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.707+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 19650 for partition rappel_conso-0
[2024-04-25T13:33:22.714+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.724+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.725+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.726+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.757+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 20136 for partition rappel_conso-0
[2024-04-25T13:33:22.761+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.766+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.767+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.768+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.790+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 20620 for partition rappel_conso-0
[2024-04-25T13:33:22.793+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.795+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.795+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.798+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.828+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 21120 for partition rappel_conso-0
[2024-04-25T13:33:22.831+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:22.837+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:22.837+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:22.840+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to offset 21125 for partition rappel_conso-0
[2024-04-25T13:33:22.842+0000] {docker.py:413} INFO - 24/04/25 13:33:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to earliest offset of partition rappel_conso-0
[2024-04-25T13:33:23.345+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:23.346+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Seeking to latest offset of partition rappel_conso-0
[2024-04-25T13:33:23.347+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting offset for partition rappel_conso-0 to position FetchPosition{offset=21156, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=0}}.
[2024-04-25T13:33:23.451+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO KafkaDataConsumer: From Kafka topicPartition=rappel_conso-0 groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor read 21156 records through 74 polls (polled  out 21156 records), taking 1710014775 nanos, during time span of 3946155134 nanos.
[2024-04-25T13:33:23.456+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2401 bytes result sent to driver
[2024-04-25T13:33:23.460+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4591 ms on localhost (executor driver) (1/1)
[2024-04-25T13:33:23.460+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-04-25T13:33:23.463+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO DAGScheduler: ShuffleMapStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 4.681 s
[2024-04-25T13:33:23.464+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO DAGScheduler: looking for newly runnable stages
[2024-04-25T13:33:23.465+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO DAGScheduler: running: Set()
[2024-04-25T13:33:23.466+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2024-04-25T13:33:23.467+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO DAGScheduler: failed: Set()
[2024-04-25T13:33:23.469+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[18] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-04-25T13:33:23.556+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 89.0 KiB, free 434.2 MiB)
[2024-04-25T13:33:23.560+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.2 MiB)
[2024-04-25T13:33:23.561+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:45005 (size: 34.9 KiB, free: 434.3 MiB)
[2024-04-25T13:33:23.564+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
[2024-04-25T13:33:23.570+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[18] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-04-25T13:33:23.571+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks resource profile 0
[2024-04-25T13:33:23.585+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (localhost, executor driver, partition 0, NODE_LOCAL, 10414 bytes)
[2024-04-25T13:33:23.587+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (localhost, executor driver, partition 1, NODE_LOCAL, 10414 bytes)
[2024-04-25T13:33:23.589+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4) (localhost, executor driver, partition 2, NODE_LOCAL, 10414 bytes)
[2024-04-25T13:33:23.591+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5) (localhost, executor driver, partition 3, NODE_LOCAL, 10414 bytes)
[2024-04-25T13:33:23.604+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO Executor: Running task 2.0 in stage 2.0 (TID 4)
[2024-04-25T13:33:23.605+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-04-25T13:33:23.606+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
[2024-04-25T13:33:23.625+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO Executor: Running task 3.0 in stage 2.0 (TID 5)
[2024-04-25T13:33:23.769+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Getting 1 (61.8 KiB) non-empty blocks including 1 (61.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:23.770+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Getting 1 (61.8 KiB) non-empty blocks including 1 (61.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:23.780+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
[2024-04-25T13:33:23.786+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Getting 1 (56.2 KiB) non-empty blocks including 1 (56.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:23.787+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 36 ms
[2024-04-25T13:33:23.788+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Getting 1 (61.8 KiB) non-empty blocks including 1 (61.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:23.788+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 32 ms
[2024-04-25T13:33:23.802+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO CodeGenerator: Code generated in 17.028362 ms
[2024-04-25T13:33:23.837+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO CodeGenerator: Code generated in 24.664571 ms
[2024-04-25T13:33:23.872+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO CodeGenerator: Code generated in 20.0673 ms
[2024-04-25T13:33:23.888+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:23.889+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:23.889+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2024-04-25T13:33:23.894+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2024-04-25T13:33:23.897+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:23.898+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2024-04-25T13:33:23.898+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:23.898+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
[2024-04-25T13:33:23.909+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO CodeGenerator: Code generated in 18.205352 ms
[2024-04-25T13:33:23.989+0000] {docker.py:413} INFO - 24/04/25 13:33:23 INFO CodeGenerator: Code generated in 71.57623 ms
[2024-04-25T13:33:26.056+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO CodeGenerator: Code generated in 53.147093 ms
[2024-04-25T13:33:26.340+0000] {docker.py:413} INFO - 24/04/25 13:33:26 ERROR Executor: Exception in task 3.0 in stage 2.0 (TID 5)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-05-0251','https://rappel.conso.gouv.fr/image/15663247-9e6f-4715-853b-d2ea76407111.jpg https://rappel.conso.gouv.fr/image/c7dc09f2-6f38-4e84-a7ae-35ba04ecedf3.jpg https://rappel.conso.gouv.fr/image/24e6ff86-ae9d-4946-b9ad-242dc4292fc6.jpg https://rappel.conso.gouv.fr/image/debd0f64-d659-4e14-aa49-136bfafc4f10.jpg https://rappel.conso.gouv.fr/image/3f18c5a7-c186-45d2-a715-1a3ef2ed08f4.jpg',NULL,'https://rappel.conso.gouv.fr/document/1ec27d91-dec3-410c-ad5a-90434ffd881e/Interne/ListeDesDistributeurs','https://rappel.conso.gouv.fr/affichettePDF/565/Interne','https://rappel.conso.gouv.fr/fiche-rappel/565/Interne','2021-06-02','mardi 8 juin 2021','Alimentation','Lait et produits laitiers','FERME DE LA VIEILLE RUE','Fromage blanc fermier, formage frais fermier campagne , fromage blanc battu myrtille , formage blanc battu vanille , fromage blanc battu fraise','FAB:18052021 Date limite de consommation 08/06/2021','pot 500gr','Produit a conserver au refrigerateur','Departements: OISE (60)','Cf: liste revendeurs','Presence de listeria <10',NULL,'Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer','19/05/2021','22/05/2021',NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-05-0251) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-05-0251) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:26.343+0000] {docker.py:413} INFO - 24/04/25 13:33:26 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 4)
java.sql.BatchUpdateException: Batch entry 2 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-05-0183','https://rappel.conso.gouv.fr/image/f8feb1d0-a5ec-4381-bb58-4f36e773c8e0.jpg',NULL,'https://rappel.conso.gouv.fr/document/abdbfd8a-6104-467e-a710-399815c35c36/Interne/ListeDesDistributeurs','https://rappel.conso.gouv.fr/affichettePDF/496/Interne','https://rappel.conso.gouv.fr/fiche-rappel/496/Interne','2021-05-21',NULL,'Alimentation','Aliments dietetiques et nutrition','First Iron Systems','Rip Cut 24h Stage 2 - 80 gelules. 
Numero du lot : F02707
Date de durabilite minimale :01/02/2024','F02707 Date de durabilite minimale 01/02/2024','Conditionnement dans une boite en carton. 
Poids : 98,6 g.','Produit a conserver a temperature ambiante','France entiere','Produit vendu par ligne par L&S SRL - 79 rue du romarin 7782 Ploegsteert via : https://www.first-iron-systems.com/language/fr/
La liste des points de vente en France est jointe a la presente alerte.','Depassement du taux maximal autorise d''oxyde d''ethylene dans une matiere premiere entrant dans la composition du produit.',NULL,'Autre (voir informations complementaires)','Presence de substance interdite','Ne plus consommer Contacter le point de vente','02/03/2020','07/05/2021',NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-05-0183) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-05-0183) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:26.345+0000] {docker.py:413} INFO - 24/04/25 13:33:26 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:26.365+0000] {docker.py:413} INFO - 24/04/25 13:33:26 ERROR Executor: Exception in task 1.0 in stage 2.0 (TID 3)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-04-0192','https://rappel.conso.gouv.fr/image/809f54df-cbe4-416d-8932-ee59811bfa9f.jpg',NULL,'https://rappel.conso.gouv.fr/document/4c5ad2db-3d0d-4e43-9313-3905444c0918/Interne/ListeDesDistributeurs','https://rappel.conso.gouv.fr/affichettePDF/252/Interne','https://rappel.conso.gouv.fr/fiche-rappel/252/Interne','2021-06-04','mercredi 19 mai 2021','Alimentation','Viandes','MAISON CLEMENS','CHIPOLATAS VIANDE DE PORC FRANCAISE','2961375018602 21151607 Date limite de consommation 28/04/2021','Produit vendu en Vrac au rayon TRAD','Produit a conserver au refrigerateur','France entiere','E.leclerc Bruges','Analyses bacteriologiques : presence de Salmonelles','0556161000','Remboursement','Salmonella spp (agent responsable de la salmonellose)
Les toxi-infections alimentaires causees par les salmonelles se traduisent par des troubles gastro-intestinaux (diarrhee, vomissements) d''apparition brutale souvent accompagnes de fievre et de maux de','Les toxi-infections alimentaires causees par les salmonelles se traduisent par des troubles gastro-intestinaux (diarrhee, vomissements) d''apparition brutale souvent accompagnes de fievre et de maux de tete qui surviennent generalement 6h a 72h apres la consommation des produits contamines.  Ces symptomes peuvent etre plus prononces chez les jeunes enfants, les femmes enceintes, les sujets immunodeprimes et les personnes agees. Les personnes qui auraient consomme ces produits et qui presenteraient ces symptomes sont invitees a consulter leur medecin traitant en lui signalant cette consommation. En l''absence de symptomes dans les 7 jours apres la consommation des produits concernes, il est inutile de s''inquieter et de consulter un medecin. Si le produit doit subir une cuisson avant consommation : la cuisson a coeur des produits (oeufs durs, patisseries, viandes de volailles...) a +65degC permet de detruire ces bacteries et de prevenir les consequences d''une telle contamination.
Rapporter le produit au point de vente','04/04/2021','18/04/2021',NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-04-0192) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-04-0192) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:26.383+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 6) (localhost, executor driver, partition 4, NODE_LOCAL, 10414 bytes)
[2024-04-25T13:33:26.384+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Running task 4.0 in stage 2.0 (TID 6)
[2024-04-25T13:33:26.394+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 7) (localhost, executor driver, partition 5, NODE_LOCAL, 10414 bytes)
[2024-04-25T13:33:26.396+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Running task 5.0 in stage 2.0 (TID 7)
[2024-04-25T13:33:26.398+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 8) (localhost, executor driver, partition 6, NODE_LOCAL, 10414 bytes)
[2024-04-25T13:33:26.400+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Running task 6.0 in stage 2.0 (TID 8)
[2024-04-25T13:33:26.421+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 9) (localhost, executor driver, partition 7, NODE_LOCAL, 10414 bytes)
[2024-04-25T13:33:26.424+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Running task 7.0 in stage 2.0 (TID 9)
[2024-04-25T13:33:26.436+0000] {docker.py:413} INFO - 24/04/25 13:33:26 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:26.452+0000] {docker.py:413} INFO - 24/04/25 13:33:26 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
[2024-04-25T13:33:26.453+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO TaskSchedulerImpl: Cancelling stage 2
[2024-04-25T13:33:26.454+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Getting 1 (74.8 KiB) non-empty blocks including 1 (74.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:26.455+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2024-04-25T13:33:26.458+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.462+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Getting 1 (61.8 KiB) non-empty blocks including 1 (61.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:26.466+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Getting 1 (74.8 KiB) non-empty blocks including 1 (74.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:26.472+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2024-04-25T13:33:26.476+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO TaskSchedulerImpl: Stage 2 was cancelled
[2024-04-25T13:33:26.477+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2024-04-25T13:33:26.477+0000] {docker.py:413} INFO - 24/04/25 13:33:26 WARN TaskSetManager: Lost task 2.0 in stage 2.0 (TID 4) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-05-0183','https://rappel.conso.gouv.fr/image/f8feb1d0-a5ec-4381-bb58-4f36e773c8e0.jpg',NULL,'https://rappel.conso.gouv.fr/document/abdbfd8a-6104-467e-a710-399815c35c36/Interne/ListeDesDistributeurs','https://rappel.conso.gouv.fr/affichettePDF/496/Interne','https://rappel.conso.gouv.fr/fiche-rappel/496/Interne','2021-05-21',NULL,'Alimentation','Aliments dietetiques et nutrition','First Iron Systems','Rip Cut 24h Stage 2 - 80 gelules. 
Numero du lot : F02707
Date de durabilite minimale :01/02/2024','F02707 Date de durabilite minimale 01/02/2024','Conditionnement dans une boite en carton. 
Poids : 98,6 g.','Produit a conserver a temperature ambiante','France entiere','Produit vendu par ligne par L&S SRL - 79 rue du romarin 7782 Ploegsteert via : https://www.first-iron-systems.com/language/fr/
La liste des points de vente en France est jointe a la presente alerte.','Depassement du taux maximal autorise d''oxyde d''ethylene dans une matiere premiere entrant dans la composition du produit.',NULL,'Autre (voir informations complementaires)','Presence de substance interdite','Ne plus consommer Contacter le point de vente','02/03/2020','07/05/2021',NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-05-0183) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-05-0183) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:26.478+0000] {docker.py:413} INFO - 24/04/25 13:33:26 WARN TaskSetManager: Lost task 3.0 in stage 2.0 (TID 5) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-05-0251','https://rappel.conso.gouv.fr/image/15663247-9e6f-4715-853b-d2ea76407111.jpg https://rappel.conso.gouv.fr/image/c7dc09f2-6f38-4e84-a7ae-35ba04ecedf3.jpg https://rappel.conso.gouv.fr/image/24e6ff86-ae9d-4946-b9ad-242dc4292fc6.jpg https://rappel.conso.gouv.fr/image/debd0f64-d659-4e14-aa49-136bfafc4f10.jpg https://rappel.conso.gouv.fr/image/3f18c5a7-c186-45d2-a715-1a3ef2ed08f4.jpg',NULL,'https://rappel.conso.gouv.fr/document/1ec27d91-dec3-410c-ad5a-90434ffd881e/Interne/ListeDesDistributeurs','https://rappel.conso.gouv.fr/affichettePDF/565/Interne','https://rappel.conso.gouv.fr/fiche-rappel/565/Interne','2021-06-02','mardi 8 juin 2021','Alimentation','Lait et produits laitiers','FERME DE LA VIEILLE RUE','Fromage blanc fermier, formage frais fermier campagne , fromage blanc battu myrtille , formage blanc battu vanille , fromage blanc battu fraise','FAB:18052021 Date limite de consommation 08/06/2021','pot 500gr','Produit a conserver au refrigerateur','Departements: OISE (60)','Cf: liste revendeurs','Presence de listeria <10',NULL,'Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer','19/05/2021','22/05/2021',NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-05-0251) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-05-0251) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:26.478+0000] {docker.py:413} INFO - 24/04/25 13:33:26 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 3) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-04-0192','https://rappel.conso.gouv.fr/image/809f54df-cbe4-416d-8932-ee59811bfa9f.jpg',NULL,'https://rappel.conso.gouv.fr/document/4c5ad2db-3d0d-4e43-9313-3905444c0918/Interne/ListeDesDistributeurs','https://rappel.conso.gouv.fr/affichettePDF/252/Interne','https://rappel.conso.gouv.fr/fiche-rappel/252/Interne','2021-06-04','mercredi 19 mai 2021','Alimentation','Viandes','MAISON CLEMENS','CHIPOLATAS VIANDE DE PORC FRANCAISE','2961375018602 21151607 Date limite de consommation 28/04/2021','Produit vendu en Vrac au rayon TRAD','Produit a conserver au refrigerateur','France entiere','E.leclerc Bruges','Analyses bacteriologiques : presence de Salmonelles','0556161000','Remboursement','Salmonella spp (agent responsable de la salmonellose)
Les toxi-infections alimentaires causees par les salmonelles se traduisent par des troubles gastro-intestinaux (diarrhee, vomissements) d''apparition brutale souvent accompagnes de fievre et de maux de','Les toxi-infections alimentaires causees par les salmonelles se traduisent par des troubles gastro-intestinaux (diarrhee, vomissements) d''apparition brutale souvent accompagnes de fievre et de maux de tete qui surviennent generalement 6h a 72h apres la consommation des produits contamines.  Ces symptomes peuvent etre plus prononces chez les jeunes enfants, les femmes enceintes, les sujets immunodeprimes et les personnes agees. Les personnes qui auraient consomme ces produits et qui presenteraient ces symptomes sont invitees a consulter leur medecin traitant en lui signalant cette consommation. En l''absence de symptomes dans les 7 jours apres la consommation des produits concernes, il est inutile de s''inquieter et de consulter un medecin. Si le produit doit subir une cuisson avant consommation : la cuisson a coeur des produits (oeufs durs, patisseries, viandes de volailles...) a +65degC permet de detruire ces bacteries et de prevenir les consequences d''une telle contamination.
Rapporter le produit au point de vente','04/04/2021','18/04/2021',NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-04-0192) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-04-0192) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:26.479+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Executor is trying to kill task 7.0 in stage 2.0 (TID 9), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.480+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Executor is trying to kill task 4.0 in stage 2.0 (TID 6), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.480+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Executor is trying to kill task 5.0 in stage 2.0 (TID 7), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.481+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Executor is trying to kill task 6.0 in stage 2.0 (TID 8), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.481+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Getting 1 (51.1 KiB) non-empty blocks including 1 (51.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:26.481+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2024-04-25T13:33:26.482+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) failed in 2.949 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.487+0000] {docker.py:413} INFO - 24/04/25 13:33:26 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks, null
[2024-04-25T13:33:26.489+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:26.490+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2024-04-25T13:33:26.494+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:26.500+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2024-04-25T13:33:26.506+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO DAGScheduler: Job 0 failed: start at NativeMethodAccessorImpl.java:0, took 7.986145 s
[2024-04-25T13:33:26.526+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Executor interrupted and killed task 4.0 in stage 2.0 (TID 6), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.539+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-04-25T13:33:26.540+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2024-04-25T13:33:26.559+0000] {docker.py:413} INFO - 24/04/25 13:33:26 WARN TaskSetManager: Lost task 4.0 in stage 2.0 (TID 6) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
[2024-04-25T13:33:26.579+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Executor killed task 6.0 in stage 2.0 (TID 8), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.606+0000] {docker.py:413} INFO - 24/04/25 13:33:26 WARN TaskSetManager: Lost task 6.0 in stage 2.0 (TID 8) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
[2024-04-25T13:33:26.625+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Executor killed task 7.0 in stage 2.0 (TID 9), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.629+0000] {docker.py:413} INFO - 24/04/25 13:33:26 WARN TaskSetManager: Lost task 7.0 in stage 2.0 (TID 9) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
[2024-04-25T13:33:26.647+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Executor: Executor killed task 5.0 in stage 2.0 (TID 7), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-04-25T13:33:26.653+0000] {docker.py:413} INFO - 24/04/25 13:33:26 WARN TaskSetManager: Lost task 5.0 in stage 2.0 (TID 7) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
[2024-04-25T13:33:26.654+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-04-25T13:33:26.900+0000] {docker.py:413} INFO - 2024-04-25 13:33:26,841:_call_proxy:ERROR:There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/bitnami/spark/spark_streaming.py", line 85, in <lambda>
    .write.jdbc(
           ^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o69.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:26.960+0000] {docker.py:413} INFO - 24/04/25 13:33:26 ERROR MicroBatchExecution: Query [id = 0744d6c1-ff49-4e4b-a6e1-1613983062db, runId = 5bcdc2a4-dc27-4873-948e-2bf8e3e9a4b4] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/bitnami/spark/spark_streaming.py", line 85, in <lambda>
    .write.jdbc(
           ^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o69.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD
[2024-04-25T13:33:26.961+0000] {docker.py:413} INFO - .scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geo
[2024-04-25T13:33:26.961+0000] {docker.py:413} INFO - graphique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more


	at py4j.Protocol.getReturnValue(Protocol.java:476)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2024-04-25T13:33:26.964+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2024-04-25T13:33:26.971+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Metrics: Metrics scheduler closed
[2024-04-25T13:33:26.974+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-04-25T13:33:26.975+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO Metrics: Metrics reporters closed
[2024-04-25T13:33:26.982+0000] {docker.py:413} INFO - 24/04/25 13:33:26 INFO MicroBatchExecution: Async log purge executor pool for query [id = 0744d6c1-ff49-4e4b-a6e1-1613983062db, runId = 5bcdc2a4-dc27-4873-948e-2bf8e3e9a4b4] has been shutdown
[2024-04-25T13:33:27.197+0000] {docker.py:413} INFO - Traceback (most recent call last):
  File "/opt/bitnami/spark/spark_streaming.py", line 103, in <module>
[2024-04-25T13:33:27.198+0000] {docker.py:413} INFO - write_to_postgres()
  File "/opt/bitnami/spark/spark_streaming.py", line 99, in write_to_postgres
    start_streaming(df_final, spark=spark)
  File "/opt/bitnami/spark/spark_streaming.py", line 92, in start_streaming
[2024-04-25T13:33:27.199+0000] {docker.py:413} INFO - return query.awaitTermination()
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2024-04-25T13:33:27.199+0000] {docker.py:413} INFO - File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2024-04-25T13:33:27.199+0000] {docker.py:413} INFO - pyspark.errors.exceptions.captured.StreamingQueryException
[2024-04-25T13:33:27.215+0000] {docker.py:413} INFO - : [STREAM_FAILED] Query [id = 0744d6c1-ff49-4e4b-a6e1-1613983062db, runId = 5bcdc2a4-dc27-4873-948e-2bf8e3e9a4b4] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/bitnami/spark/spark_streaming.py", line 85, in <lambda>
    .write.jdbc(
           ^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o69.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 1 INSERT INTO rappel_conso_table ("reference_fiche","liens_vers_les_images","lien_vers_la_liste_des_produits","lien_vers_la_liste_des_distributeurs","lien_vers_affichette_pdf","lien_vers_la_fiche_rappel","date_de_publication","date_de_fin_de_la_procedure_de_rappel","categorie_de_produit","sous_categorie_de_produit","nom_de_la_marque_du_produit","noms_des_modeles_ou_references","identification_des_produits","conditionnements","temperature_de_conservation","zone_geographique_de_vente","distributeurs","motif_du_rappel","numero_de_contact","modalites_de_compensation","risques_pour_le_consommateur","recommandations_sante","date_debut_commercialisation","date_fin_commercialisation","informations_complementaires") VALUES ('2021-03-0012','https://rappel.conso.gouv.fr/image/6d8c2e42-8f73-48be-990e-236ffc464dd9.jpg',NULL,NULL,'https://rappel.conso.gouv.fr/affichettePDF/21/Interne','https://rappel.conso.gouv.fr/fiche-rappel/21/Interne','2021-03-31','samedi 22 mai 2021','Alimentation','Lait et produits laitiers','NOS REGIONS ONT DU TALENT','VALENCAY AU LAIT CRU DE CHEVRE APPELLATION D''ORIGINE PROTEGEE 220 g','3564709006031 V028 Date de durabilite minimale 04/04/2021','blister transparent','Produit a conserver au refrigerateur','France entiere','E. LECLERC','presence de LISTERIA MONOCYTOGENES','0800874187','Remboursement','Listeria monocytogenes (agent responsable de la listeriose)','Les personnes qui auraient consomme les << produits >> mentionnes ci-dessus et qui presenteraient de la fievre, isolee ou accompagnee de maux de tete, et des courbatures, sont invitees a consulter leur medecin traitant en lui signalant cette consommation.  Des formes graves avec des complications neurologiques et des atteintes maternelles ou foetales chez la femme enceinte peuvent egalement parfois survenir. Les femmes enceintes ainsi que les personnes immunodeprimees et les personnes agees doivent etre particulierement attentives a ces symptomes. La listeriose est une maladie qui peut etre grave et dont le delai d''incubation peut aller jusqu''a huit semaines.
Ne plus consommer Ne plus utiliser le produit Rapporter le produit au point de vente Contacter le service consommateur','17/02/2021',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "rappel_conso_table_pkey"
  Detail: Key (reference_fiche)=(2021-03-0012) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-04-25T13:33:27.216+0000] {docker.py:413} INFO - 2024-04-25 13:33:27,212:close:INFO:Closing down clientserver connection
[2024-04-25T13:33:27.298+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/04/25 13:33:27 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1, groupId=spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor] Request joining group due to: consumer pro-actively leaving the group
[2024-04-25T13:33:27.299+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO Metrics: Metrics scheduler closed
24/04/25 13:33:27 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-04-25T13:33:27.299+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO Metrics: Metrics reporters closed
[2024-04-25T13:33:27.305+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-002be2d2-b3b8-4140-8cfe-f54fa56e36e5-990603874-executor-1 unregistered
[2024-04-25T13:33:27.306+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO SparkContext: Invoking stop() from shutdown hook
[2024-04-25T13:33:27.307+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2024-04-25T13:33:27.321+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
[2024-04-25T13:33:27.344+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-04-25T13:33:27.374+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO MemoryStore: MemoryStore cleared
[2024-04-25T13:33:27.375+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO BlockManager: BlockManager stopped
[2024-04-25T13:33:27.380+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-04-25T13:33:27.386+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-04-25T13:33:27.405+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO SparkContext: Successfully stopped SparkContext
[2024-04-25T13:33:27.405+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO ShutdownHookManager: Shutdown hook called
[2024-04-25T13:33:27.406+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27/pyspark-c1f75bcc-7345-45e0-8dea-2861ff64c1e7
[2024-04-25T13:33:27.414+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-150dd536-c20f-46f4-a947-97ac7f4a77a0
[2024-04-25T13:33:27.422+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a1ccaa0-05bc-4a58-8dd7-d63717938c27
[2024-04-25T13:33:27.428+0000] {docker.py:413} INFO - 24/04/25 13:33:27 INFO ShutdownHookManager: Deleting directory /tmp/temporary-bb4f5896-723b-48c4-98a4-f97eed10aa09
[2024-04-25T13:33:28.510+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 268, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http://docker-proxy:2375/v1.45/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 348, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 375, in _run_image_with_mounts
    self.container = self.cli.create_container(
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/container.py", line 431, in create_container
    return self.create_container_from_config(config, name, platform)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/container.py", line 448, in create_container_from_config
    return self._result(res, True)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 274, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 270, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http://docker-proxy:2375/v1.45/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /tmp/airflowtmp14blk3or")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 486, in execute
    return self._run_image()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 357, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 421, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-04-25T13:33:28.516+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=kafka_spark_dag, task_id=pyspark_consumer, execution_date=20240425T133251, start_date=20240425T133256, end_date=20240425T133328
[2024-04-25T13:33:28.534+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 21 for task pyspark_consumer (Docker container failed: {'StatusCode': 1}; 3073)
[2024-04-25T13:33:28.567+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-04-25T13:33:28.590+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
